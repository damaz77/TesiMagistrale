\section{Preliminary background}
Let us consider the following optimization problem:
\begin{subequations}\label{eq:problem} 
\begin{align}
\min_{x,y} & \quad f(x,y)  \\
\text{s.t.} & \quad l \leq x \leq u \\
& \quad \mathds{1}^T x = 1 
\end{align}
\end{subequations}
where $x \in \R^n$, $y \in \R^m$, $f$ continuously differentiable, $l, u \in \R^n$ with $l < u$.\\
We define the feasible set $\mathcal{F}$  of Problem (\ref{eq:problem}):
\begin{equation}
\mathcal{F} = \{(x,y) \in \R^{n+m} : \mathds{1}^T x = 1, l \leq x \leq u\}.
\end{equation}
Since the constraints of Problem (\ref{eq:problem}) respect constraints qualification conditions, a point $(x,y) \in \mathcal{F}$ is critical, if the Karush-Kuhn-Tucker (KKT) conditions are satisfied. Let $L(x,y,\lambda,\mu,\gamma)$ the Lagrangian function associated to Problem (\ref{eq:problem}) then we can write KKT conditions.

\begin{proposition}[Optimality conditions (Necessary)]\label{prop:KKT}

Let $(x^*,y^*) \in \R^{n+m}$, with $(x^*,y^*) \in \mathcal{F}$, a local optimum for Problem (\ref{eq:problem}). Then there exist three multipliers $\lambda^* \in \R^n$, $\mu^* \in \R^n, \gamma^* \in \R$ such that:
\begin{equation}
 \begin{aligned}
  &\nabla_x L(x^*,y^*\lambda^*,\mu^*,\gamma^*)= \nabla_x f(x^*,y^*)+\lambda^*-\mu^*+\gamma^*=0\\
 &\nabla_y L(x^*,y^*,\lambda^*,\mu^*,\gamma^*)=\nabla_y f(x^*,y^*) =0 \\
    &\lambda^*_i(l_i-x_i^*)=0,\ \forall i\\
 &\mu^*_i(x_i^*-u_i)=0,\ \forall i\\
   & \lambda^*,\mu^*\ge0 \\
 \end{aligned}
\end{equation}
\end{proposition}

From the first condition we have:
\begin{equation}
 \nabla_x f(x^*,y^*)-\lambda^*+\mu^*+\gamma^*=0
\end{equation}

Then there are three possible cases:
\begin{equation}
 \frac{\partial f(x^*,y^*)}{dx_i} = \begin{cases} -\mu_i^* -\gamma^* \hspace{1cm} x^*_i =u \\
 -\gamma^*+\lambda^*_i \hspace{1cm} x^*_i =l \\
 -\gamma^* \hspace{1.65cm} l<x^*_i <u 
\end{cases}
\end{equation}
Then if $x^*_i>l_i$: 
\begin{equation}
 \frac{\partial f(x^*,y^*)}{dx_i} \le \frac{\partial f(x^*,y^*)}{dx_h}, \forall h
\end{equation}

After writing KKT optimality condition we focus on feasible direction in a feasibile point.

A vector $d\in \R^{n+m}$ is partitioned as follows
$$
d=\left(
\begin{array}{c}
d_x\\
d_y
\end{array}
\right ),
$$
where $d_x\in R^n$ and $d_y\in R^m$.

Given $(x,y) \in \mathcal{F}$, the set of feasible directions in $(x,y)$ is the cone
\begin{equation}
 \mathcal{D}(x,y)=\{ d \in \R^{n+m}: \mathds{1}^Td_x=0, d_i\ge 0 \ \forall i \in L(x), d_i\le 0 \ \forall i \in U(x)\}
\end{equation}
where
\begin{equation}
 \begin{aligned}
  &L(x)=\{ i: \ x_i=l_i\}\\
  &U(x)=\{ i: \ x_i=u_i\}
 \end{aligned}
\end{equation}
Given $(\bar x,\bar y) \in \mathcal{F}$, we say that $(\bar x,\bar y)$ is a {\it critical point} if
$$
\nabla f(\bar x,\bar y)^Td\ge 0\quad\quad \forall d\in  \mathcal{D}(\bar x,\bar y).
$$
We can state the following result.
\begin{proposition}\label{optimality}
A point $(\bar x,\bar y) \in \mathcal{F}$ is a critical point if and only if
\begin{equation}\label{on_x}
\begin{aligned}
&\nabla_xf(\bar x,\bar y)^Td_x\ge 0 \quad \forall d_x\in R^n & \\ 
&\text{s.t.} \quad \mathds{1}^Td_x=0,\quad d_i\ge 0 \ \forall i \in L(\bar x), \quad d_i\le 0 \ \forall i \in U(\bar x)&
\end{aligned}
\end{equation}
\begin{equation}\label{on_y}
 \nabla_yf(\bar x,\bar y)=0.
\end{equation} 
\end{proposition}

In correspondence to a feasible point $(x,y)$ we introduce the index sets
$$
R(x)=L(x)\cup C(x)
$$
$$
S(x)=U(x)\cup C(x),
$$
where 
$$
C(x)=\{i: l_i<x_i<u_i\}.
$$
We can state the following propositions (Propositions 2.2 and 2.3 in Jota2009).
\begin{proposition}\label{2.2}
 A feasible point $(\bar x,\bar y)$ is a critical point if and only if for each pair $(i,j)$,
$i\in R(\bar x)$, $j\in S(\bar x)$, we have
\begin{equation}\label{on_RS}
 {{\partial f(\bar x,\bar y)}\over{\partial x_i}}\ge
 {{\partial f(\bar x,\bar y)}\over{\partial x_j}}
\end{equation}
\begin{equation}\label{on_y2}
\nabla_yf(\bar x,\bar y)=0.
\end{equation}
\end{proposition}
\begin{proposition}\label{2.3}
 Let $\{(x^k,y^k)\}$ be a sequence of feasible points convergent to a point $(\bar x,\bar y)$.
Then, for sufficiently large values of $k$, we have
$$
R(\bar x)\subseteq R(x^k) \quad \quad {\rm and}\quad \quad S(\bar x)\subseteq S(x^k).
$$
\end{proposition}

\subsection{Set of sparse feasible directions}
In our decomposition framework we will employ feasible directions having only two
nonzero components.
Then, in this subsection we introduce these sparse feasible directions and we show their important properties.

Given $i, j\in  \{1, \ldots ,n\}$, with $i\ne j$,
we indicate by $d^{i,j}\in  \R^{n+m}$ such that
\begin{equation}\label{eq:direction}
d_h^{i,j}= 
\begin{cases}
1, \quad \text{    } h=i\\
-1, \text{    } \text{    } h=j\\
0, \quad \text{    } \text{otherwise}
\end{cases}
\end{equation}
Note that, by definition, the components $d_h^{i,j}$ with $h=n+1,\ldots,n+m$ are always set to zero.

Given $(x, y) \in \mathcal{F}$ and the corresponding index sets $R(x)$ and $S(x)$, we indicate by $D_{RS}(x,y)$
the set of directions $d^{i,j}$ with $i \in R(x)$ and $j \in S(x)$, namely
$$
D_{RS}(x,y)=\cup_{i\in R(x),j\in S(x)}d^{i,j}.
$$
\begin{proposition}\label{3.1}
Let $(\bar x,\bar y)$ be a feasible point. For each pair $i \in R(x)$ and $j \in S(x)$, the
direction $d^{i,j}\in \R^{n+m}$ is a feasible direction at $(\bar x,\bar y)$, i.e. $d \in D(\bar x,\bar y)$.
\end{proposition}
\begin{proposition}\label{3.2}
A feasible point $(\bar x,\bar y)$
 is a critical point if and only
\begin{equation}\label{on_x2}
\nabla f(\bar x,\bar y)^Td^{i,j}\ge 0\quad\quad \forall d^{i,j}\in D_{RS}(\bar x,\bar y)
\end{equation}
\begin{equation}\label{on_y3}
 \nabla_y f(\bar x,\bar y)=0.
\end{equation} 
\end{proposition}
Given a feasible point $(\bar x,\bar y)$, a pair $i\in R(\bar x)$ and $j\in S(\bar x)$ such that
$$
\nabla f(\bar x,\bar y)^Td^{i,j}<0
$$
is said a {\it Violating Pair} (VP).

A violating pair $(i^\star,j^\star)$ such that 
\begin{equation}\label{mvp}
 \nabla f(\bar x,\bar y)^Td^{i^\star,j^\star}\le \nabla f(\bar x,\bar y)^Td^{i,j} \quad \forall i\in R(\bar x), \ j\in S(\bar x).
\end{equation}
is the {\it Most Violating Pair} (MVP).

\subsection{Armijo-Type Line Search Algorithm}
In this section, we briefly describe the well-known Armijo-type line search along a feasible descent direction. 
Let $d^{k} \in \mathcal{D}(x_k)$, $x^{k} \in \mathcal{F}$. We denote by $\Delta_{k}$ the maximum feasible step along $d^{k}$. It is easy to see that:
\begin{equation*}
\Delta_k= \min \{ x^k_{j(k)}-l_{j(k)}, u_{i(k)}-x^k_{i(k)}\}
\end{equation*}

\begin{algorithm}[ht]
 \KwData{Given $\alpha > 0$, $\delta \in (0,1)$, $\gamma \in (0, 1/2)$ and the initial stepsize $\Delta^{(k)} =\min \{ x^k_{j(k)}-l, u-x^k_{i(k)}\}$ }
 %\KwResult{A feasible step $\lambda$}
 Set $\alpha = \Delta^{(k)}$\\
 \While{$f(x^{k},y^k) + \alpha d^{k}) > f(x^{k},y^k) + \gamma \alpha \nabla_x f(x^{k},y^k)^T d^{k}$}{
  Set $\alpha = \delta \alpha$
 }
 \caption{Armijo-Type Line Search}
\end{algorithm}

Then at iteration $k+1$ we have:
\begin{equation*}
x^{k+1}_{j(k)}=\begin{cases}
 l_{j(k)} \ &\alpha_k=x^k_{j(k)}-l_{j(k)}\\
 x^k_{j(k)}-u_{i(k)} +x^k_{i(k)} \ &\alpha_k=u_{i(k)} -x^k_{i(k)}
 \end{cases}
\end{equation*}
and:
\begin{equation*}
x^{k+1}_{i(k)}=\begin{cases}
 x^k_{j(k)}-l_{j(k)}+x^k_{i(k)} \ &\alpha_k=x^k_{j(k)}-l_{j(k)}\\
 u_{i(k)} \ &\alpha_k=u_{i(k)}-x^k_{i(k)}
 \end{cases}
\end{equation*}
\subsection{Quadratic Line Search (QLS)}
In order to find a feasible step along a descent direction in $x^k$, we use a line search method called quadratic line search. The QLS algorithm procedure (Algorithm \ref{alg:qls}) starts from $\alpha_k = \Delta_k$ and decreases $\alpha_k$ until:
\begin{equation}
f(x_k+\alpha_kd_k) \le  f(x_k)- \gamma (\alpha_k||d_k||)^2
\end{equation}
 where $d_k$ is a descent direction in $x_k$.

Altough the most famous line search method is Armijo-Type, its most important drawback is that it can't guarantee that:
\begin{equation}
 \displaystyle \lim_{k\rightarrow \infty} ||x^{k+1}-x^{k}|| =0
\end{equation}

 \begin{algorithm}[ht]
 \KwData{Given $\alpha > 0$, $\delta \in (0,1)$, $\gamma \in (0, 1/2)$ and the initial stepsize $\Delta^{k} =\min \{ x^k_{j(k)}-l, u-x^k_{i(k)}\}$ }
 %\KwResult{A feasible step $\lambda$}
 Set $\alpha = \Delta^{k}$\\
 \While{$f(x^{k},y^k) + \alpha d^{k}) > f(x^{k},y^k) - \gamma \left(\alpha ||d^{k}||\right)^2$}{
  Set $\alpha = \delta \alpha$
 }
 \caption{QLS Line Search}\label{alg:qls}
\end{algorithm}

QLS algorithm has also the same convergency properties \cite{sciandrone-galligari-dilorenzo} of Armijo-type.
\section{The decomposition framework}
As already discussed, we partition the vector of variables into two blocks in order to take into account
the structure of the feasible set and possibly the form of the objective function (see, for instance,
the formulation of the risk parity problem, where the objective function is convex w.r.t. the scalar variable $\theta$).
The first block contains the constrained variables $x$, the second block contains
the unconstrained variables $y$. 

A first possibility can be that of defining a two-blocks Gauss-Seidel algorithm.
According to this scheme, at each iteration, the two component vectors $x$ and $y$ are
sequentially updated by performing  minimization steps (either exact or inexact) by  suitable descent techniques.
Globally convergent results of Gauss-Seidel algorithms (both exact and inexact) have been established in \cite{}, \cite{}, \cite{}.

We present here a block descent algorithm where a further level of decomposition is
introduced with respect to the block component $x$. 
More specifically, at each iteration, only two variables are updated, those corresponding
to a {\it Violating Pair}, by performing an inexact line search along a feasible and descent direction.
In order to guarantee convergence, the {\it Most Violating Pair} must be selected at least periodically, say every $M$ iterations.
We will discuss in the section of the computational experiments the role and the influence of the parameter $M$.

The adoption of a decomposition strategy with respect to the subvector $x$ is suitable
whenever the number $n$ of variables is large.

Note that the properties of the standard Armijo-type line search do not guarantee, without further assumptions
on the descent search direction $d^k$, that the distance between successive points tends to zero, which is a usual requirement of decomposition methods. 
This motivates the employment of the Quadratic Line Searck (QLS) defined in Algorithm \ref{alg:proximal} and based on the acceptance condition
$$
f((x^k,y^k)+\alpha^kd^k)\le f((x^k),y^k))-\gamma (\alpha^k)^2\|d^k\|^2.
$$
Concerning the unconstrained block component $y$, we do not specify the updating rule, but we state the following assumption that
could be satisfied, in practice, by different techniques depending on the hypothesis on $f$.
\par\medskip\noindent
{\bf Assumption on the updating rule of} $y$.
\par\medskip\noindent
\begin{itemize}
\item[(i)] For each $k$ we have $f(x^k,y^{k+1})\le f(x^k,y^k)$
\item [(ii)] If
$$
\lim_{k\to\infty} \left(f(x^k,y^{k+1})- f(x^k,y^k)\right)=0
$$
then
$$
\lim_{k\to\infty}\|y^{k+1}-y^k\|=0
$$
and
$$
\lim_{k\to\infty}\nabla_y f(x^k,y^k)=0.
$$
\end{itemize}


%At every step $k$ we choose a random subset $W^k\subset \{1,..,n\}$ such as
%\begin{equation}\label{eq:lambda}
%\frac{|W^k|}{n} \times 100 = \lambda
%\end{equation}
%For each $w \in W^k$, we compute the partial derivative $\frac{\partial f(x,\theta)}{\partial x_w}$ and 
%we select the MVP among the indexes in $W^k$. If we don't find a violating pair in $W^k$, 
%we randomly add indexes until we find one, and we use this violating pair to build the descent direction. To assure the global convergence properties, we evaluate the MVP among the full gradient $\nabla_x f(x,\theta)$ every $M$ iterations. \\
The algorithm is formally described below.

\begin{algorithm}[ht]
 \KwData{Given the initial feasible point $(x^{0}, y^{0})$}
 Set $k = 0$\\
 \While{(not convergence)}{
  Compute $y^{k+1}$ such that Assumptions (i) and (ii) hold\\
 \eIf{$k \enskip \text{mod} \enskip M = 0$}
  {
  Let $(i(k), j(k))$ be a Most Violating Pair\\ 
  }
  {
  Let $(i(k), j(k))$ be a Violating Pair\ 
  }
  Compute a step $\alpha^{k}$  along the direction $d^k=d^{i(k),j(k)}$ by QLS\\
  Set $x_{i(k)}^{k+1} = x_{i(k)}^{k} + \alpha^{k}$, $x_{j(k)}^{k+1} = x_{j(k)}^{k} - \alpha^{k}$  \\

  Set $k = k + 1$
 }
 \caption{Decomposition Algorithm}
 \label{alg:decMVP}
\end{algorithm}
\par\bigskip\noindent
We can prove the following global convergence result.
\begin{proposition}
Suppose that the level set $\mathcal{L}_0$ is a compact set. Let $\{(x^k, y^k)\}$ be the sequence of points generated by the decomposition algorithm. Then
$\{(x^k, y^k)\}$ admits limit points and each limit point is critical for Problem (\ref{eq:problem}).
%\end{itemize}
\end{proposition}

\begin{proof}
The instructions of Algorithm \ref{alg:decMVP} imply
$$
f(x^{k+1}, y^{k+1})\le f(x^{k}, y^{k+1}) \leq f(x^{k}, y^{k}),
$$
so that, the points of the sequence $\{(x^{k}, y^{k})\}$ belongs to the compact set $\mathcal{L}_0$.
We also have
\begin{equation}\label{on_funct}
 \lim_{k\to\infty} f(x^k,y^{k+1})=\lim_{k\to\infty} f(x^{k},y^{k})=\bar f>-\infty
\end{equation}
Let $(\overline{x},\overline{y})$ be a limit point of $\{(x^k, y^k)\}$, i.e. there exists an infinite subset $K \subseteq N$ such that
\begin{equation}\label{eq:asim}
\lim_{k \in K, k \rightarrow \infty} (x^k, y^k) = (\overline{x},\overline{y})
\end{equation}
By contradiction, let us assume that $(\overline{x},\overline{y})$ is not a critical point. 
In this case, at least one of the following conditions holds:
\begin{subequations}
\begin{align}
&\nabla_y f(\overline{x},\overline{y}) \neq 0  \label{eq:aa}\\
&\exists \enskip i, j \enskip  \text{s.t.} \enskip d^{i,j}\in D(\bar x) \enskip  \text{and} \enskip  \nabla_x f(\overline{x},\overline{y})^T d^{i,j} = -\eta < 0 \label{eq:bb}
\end{align}
\end{subequations}
Suppose that (\ref{eq:aa}) holds.
From (\ref{on_funct}), Assumption (ii) on the updating rule of $y^k$, and the continuity of the gradient we get
$$
\lim_{k\to\infty}\nabla_y f(x^k,y^k)=\nabla_y f(\overline{x},\overline{y})=0,
$$
and this contradicts (\ref{eq:aa}).

Now assume that (\ref{eq:bb}) holds.
For each $k$, a stepsize $\alpha^k>0$ is computed by QLS along the descent direction $d^{i(k),j(k)}$.
Then we can write
\begin{equation}\label{red_funct}
 f(x^{k+1},y^{k+1})\le f(x^k,y^{k+1})-\gamma (\alpha^k)^2\|d^{i(k),j(k)}\|^2=f(x^k,y^{k+1})-\gamma\|x^{k+1}-x^k\|^2,
\end{equation}
from which, recalling (\ref{on_funct}) and that $\|d^{i(k),j(k)}\|^2=2$, we obtain
\begin{equation}\label{dst_x}
 \lim_{k\to\infty}\|x^{k+1}-x^k\|=\lim_{k\to\infty}\alpha^k=0.
\end{equation} 
From (\ref{on_funct}) and Assumption (ii) on the updating rule of $y^k$ we also have
\begin{equation}\label{dst_y}
 \lim_{k\to\infty}\|y^{k+1}-y^k\|=0.
\end{equation}
For each $k\in K$, let $v(k)$ be the integer such that $k+v(k)$ is an iteration
where the Most Violating Pair is selected. Note that we have
$$
0\le v(k) \le M.
$$
From (\ref{dst_x}) and (\ref{dst_y}) we obtain
\begin{equation}\label{cnv_x}
 \lim_{k\in K,k\to\infty}x^{k+v(k)}=\bar x
\end{equation}
\begin{equation}\label{cnv_y}
 \lim_{k\in K,k\to\infty}y^{k+v(k)+1}=\bar y.
\end{equation}
We introduce the index set 
$$
K_1=\{h: \ h=k+v(k), \ k\in K\}.
$$
By definition, for all $k\in K_1$ the Most Violating Pair $(i(k),j(k))$ is selected. Furthermore, we have
\begin{equation}\label{cnv2_x}
 \lim_{k\in K_1,k\to\infty}x^{k}=\bar x
\end{equation}
\begin{equation}\label{cnv2_y}
 \lim_{k\in K_1,k\to\infty}y^{k+1}=\bar y.
\end{equation}
Since $i(k)$ and $j(k)$ belong to the finite set $\{1,\ldots ,n\}$, we can extract a further subset (that we relabel by $K_1$) such that
$$
i(k)=i^\star \quad\quad j(k)=j^\star\quad\quad \forall k\in K_1.
$$
Note that $d^{i,j}\in D(\bar x,\bar y)$, so that, recalling Proposition \ref{2.3}, we obtain for
$k\in K_1$ and $k$ sufficiently large
\begin{equation}\label{dij_dk}
 d^{i,j}\in D(x^k,y^k).
\end{equation}
For all $k\in K_1$, as $(i^\star,j^\star)$ is the Most Violating Pair, using (\ref{dij_dk}) we can write
\begin{equation}\label{mvp_ij}
 \nabla_xf(x^k,y^{k+1})^Td^{i^\star,j^\star}\le \nabla_xf(x^k,y^{k+1})^Td^{i,j}<0.
\end{equation}
Then, $d^{i^\star,j^\star}$ is a feasible and descent direction, and hence a stepsize $\alpha^k$ is computed along it by QLS.
Further, from (\ref{dst_y}) and (\ref{eq:bb}) it follows
\begin{equation}\label{desc_mvp}
 \nabla_xf(\bar x,\bar y)^Td^{i^\star,j^\star}<0.
\end{equation}
Now let us distinguish two cases:
\par\medskip\noindent
Case (I). There exists an integer $\ell$  such that
\begin{equation}\label{caseI}
 \alpha^{k+\ell(k)}<\Delta^{k+\ell(k)}
\end{equation}
for all $k\in K_1$ and for some $\ell(k)\le \ell$.
\par\medskip\noindent 
Case (II). For all $k\in K_1$ and $m=0,\ldots ,2n$ we have
\begin{equation}\label{caseII}
 \alpha^{k+m}=\Delta^{k+m}.
\end{equation}
{\it Case} (I)
\par\medskip\noindent
Without loss of generality assume $\ell (k)=0$.
Otherwise we can reason on the subsequence $\{x^k\}_{k+l(k)}$ that, by definition,
is a subsequence where the  MVP is selected again (see the new instruction) since $\alpha^{k+l(k)-s}=\Delta^{k+l(k)-s}$,
for $s=0,1,\ldots ,l(k)$,
and $\alpha^{k+l(k)}<\Delta^{k+l(k)})$.

The instructions of QLS imply for all $k\in K_1$
\begin{equation}\label{eq:arm1}
f(x^k + \frac{\alpha^k}{\delta} d^{i^\star,j^\star}, y^{k+1}) - f(x^k,y^{k+1}) > -2\gamma \left(\frac{\alpha^k}{\delta}\right)^2 
\end{equation}
Using the Mean Value Theorem, we can write
\begin{equation}\label{eq:arm2}
f(x^k + \frac{\alpha^k}{\delta} d^{i^\star,j^\star}, y^{k+1}) = f(x^k, y^{k+1}) + \frac{\alpha^k}{\delta} \nabla_x f(z^k, y^{k+1})^T d^{i^\star,j^\star}
\end{equation}
where $z^k = x^k + \vartheta_k \frac{\alpha^k}{\delta} d^{i^\star,j^\star}$ and $\vartheta_k \in (0,1)$. 
From (\ref{eq:arm2}) and (\ref{eq:arm1}), for $k\in K_2$ and $k$ sufficiently large we have
\begin{equation}\label{eq:nabla}
\nabla_x f(z^k, y^{k+1})^T d^{i^\star,j^\star}  >- 2\gamma \frac{\alpha^k}{\delta} 
\end{equation}
Using (\ref{dst_x}) and (\ref{dst_y}) and we obtain
$$
\lim_{k \in K_1, k \rightarrow \infty} z^k = \lim_{k \in K_1, k \rightarrow \infty} x^k + \vartheta_k \frac{\alpha^k}{\delta} d^{i^\star,j^\star} = \overline{x}
$$
$$
\lim_{k\in K_1,k\to\infty}y^{k+1}=\bar y.
$$
Taking the limits in (\ref{eq:nabla}) for $k\in K_1$ and $k\to\infty$ we have
\begin{equation}\label{ddirmg0}
\nabla_x f(\overline{x},\overline{y})^T d^{i^\star,j^\star} \geq 0,
\end{equation}
which contradicts (\ref{desc_mvp}).
\par\bigskip\noindent
{\it Case} (II).
\par\medskip\noindent
For all $m=0,\ldots ,2n$ we have that at least one of these two possible cases hold
 \begin{subequations}
\begin{align}
 i(k+m)&\in R(x(k+m))\quad \quad i(k+m)\notin R(x(k+m+1))\label{eq:SetCasesA}\\
 j(k+m)&\in S(x(k+m))\quad \quad j(k+m)\notin S(x(k+m+1))\label{eq:SetCasesB}
\end{align}
\end{subequations}
Now we define two sets $\Gamma_1,\Gamma_2$ in $\{x^{k}\}_{k \in K}$ such that the first one contains all indexes $m \in\{0,\ldots,2n\}$ such that 
(\ref{eq:SetCasesA}) holds.
The second one contains all indexes $m \in\{0,\ldots,2n\}$ such that (\ref{eq:SetCasesB}) holds.

Since $|\Gamma_1|+|\Gamma_2|\ge 2n+1$, one of these sets contains a number of elements greater than $n$. 
Without loss of generality assume $|\Gamma_1|> n$.
We can say that there exist $ \hat i \in \{1,\ldots,n\}$ and $l(k),m(k)$ such that:
 \begin{equation}
  k\le l(k) <m(k)\le 2 n
 \end{equation}
and
\begin{equation}
 i(k) = i(l(k))=i(m(k))=\hat i.
\end{equation}
We can define a subset $K_1 \subseteq K$ such that $\forall k_i \in K_1$ we have
\begin{equation}
 i(k_i)=\hat i
\end{equation}
and
\begin{equation}
 k_i <k_{i+1} \le k_i+2n
\end{equation}
From the MVP rule it follows
\begin{equation}
 \frac{\partial f(x^{k_i},y^{k_i+1})}{\partial x_{\hat i}} \le \frac{\partial f(x^{k_i},y^{k_i+1})}{\partial x_{h}}, \ \forall h \in R(x^{k_i})
\end{equation}
For all $k_i \in K_1$, there exists $p(k_i)$, with  $k_i <p(k_i)<k_{i+1}$, such that
\begin{equation}
 \hat i \not \in R(x^{p(k_i)})\quad\quad \hat i \in \in R(x^{p(k_i)+1}).
\end{equation}
Then, again from the MVP rule, we must have
\begin{equation}
 \frac{\partial f(x^{p(k_i)},y^{p(k_i)+1})}{\partial x_{\hat i}} \ge \frac{\partial f(x^{p(k_i)},y^{p(k_i)+1})}{\partial x_{h}}, \ \forall h \in S(x^{p(k_i)})
\end{equation}
From (\ref{dst_x}) and (\ref{dst_y}), recalling that  $p(k_i)-k_i \le 2n$, we have
$$
 \lim_{k_i\rightarrow \infty} x^{p(k_i)}=\overline{x}
$$
$$
 \lim_{k_i\rightarrow \infty} y^{p(k_i)+1}=\overline{y}
$$
Note that $d^{i,j}\in D(\bar x,\bar y)$, so that, recalling Proposition \ref{2.3}, we obtain for
$k\in K_1$ and $k$ sufficiently large
\begin{equation}\label{dij_dka}
 d^{i,j}\in D(x^{k_i},y^{k_i+1}),
\end{equation}
\begin{equation}\label{dij_dkbis}
 d^{i,j}\in D(x^{p(k_i)},y^{p(k_i)+1}),
\end{equation}
i.e.,
$$
i\in R(x^{k_i})\quad\quad j\in S(x^{k_i})
$$ 
$$
i\in R(x^{p(k_i)})\quad\quad j\in S(x^{p(k_i)}).
$$
Then  we can write
\begin{equation}\label{eq:direction1}
\begin{aligned}
 \frac{\partial f(x^{k_i},y^{k_i+1})}{\partial x_{\hat i}} &\le \frac{\partial f(x^{k_i},y^{k_i+1})}{\partial x_{i}}\\
 \frac{\partial f(x^{p(k_i)},y^{p(k_i)+1})}{\partial x_{\hat i}} &\ge \frac{\partial f(x^{p(k_i)},y^{p(k_i)+1})}{\partial x_{j}}
 \end{aligned}
\end{equation}
Taking the limits for $k_i\in K_1$ and $k_i\to\infty$ and recalling the continuity of the gradient we obtain
$$
 \frac{\partial f(\overline{x},\overline{y})}{\partial x_i} - \frac{\partial f(\overline{x},\overline{y})}{\partial x_{j}} \ge 0,
$$
which contradicts (\ref{eq:bb}).
\end{proof}

\section{Application to Risk Parity}
Let us consider the problem
\begin{equation}\label{eq:rpfordec}
\begin{aligned}
& \underset{x, \theta}{\text{min}}
&&f(x,\theta)=\sum_{i=1}^n \left(x_i (\Sigma x)_i - \theta\right)^2\\
& \text{s.t.}
&& l \leq x \leq u\\
&&& \mathds{1}^T x = 1 \\
\end{aligned}
\end{equation}
Problem (\ref{eq:rpfordec}) is an ERC problem and it's easy to see that it is a particular case of Problem (\ref{eq:problem}). Thus we can use the decomposition algorithm just exposed to solve it.\\
$f(x,\theta)$ is clearly non-convex, but it is quadratic and strictly convex with respect to $\theta$.

\subsection{Proximal Point modification}
In this subsection we propose a \emph{Risk-Parity} version of general decomposition algorithm described above, using proximal point approximation.

Let us redefine selected variable $x_{i(k)},x_{j(k)}$ as $x_{i},x_{j}$ for ease of notation, and let us consider the step when $\theta$ is fixed (i.e $f(x,\theta)=f(x)$).

The idea behind proximal point modification is that at every iteration $k$, considering $\theta$ fixed, it's easy to solve the subproblem:
\begin{align}
 &\min_{x_i,x_j}h(x_i,x_j)= f(x_i,x_j)+ \frac{1}{2} \tau||(x_i,x_j)-(x_i^k,x_j^k)||^2\\
 &x_i+x_j = \underbrace{1-\sum_{h \ne i,j} x^k_h}_{c^k}\\
 &l \le x_i,x_j\le u
 \end{align}
where $f$ is defined in (\ref{eq:rpfordec}) and $\tau>0$.

In fact, due to simplex constraint, the objective function depends only on one of the selected variables (e.g. $x_j$) and $f$ become 4-degree polynomial in $x_j$.

Because $h$ is continuously differentiable, it admits minimum in a compact set and we have to search it between zeros of $h'(\xi)$ and the limit points of the feasible set.

Hence, at every iteration $k$, we can define the set of possible global minima as:
\begin{equation}
 O_k = \{ \xi < x_j^k: h'(\xi)=0\} \cup \{\min\{l_j,x_i^k\} \}
\end{equation}

then we set:
\begin{equation}
x_j^{k+1}= \arg \min_{\xi \in O_k} \{h(\xi)\}
\end{equation}
and:
\begin{equation}
x_i^{k+1}= c^{k}-x^{k+1}_j
\end{equation}



\begin{algorithm}[ht]
 \KwData{Given the initial feasible point $(x^{0}, \theta^{0})$}
 Set $k = 0$\\
 \While{(not convergence)}{
   Compute $\theta^{k+1}$ such that $f(x^{k},\theta^{k+1})\le f(x^{k},\theta^k)$ and $\nabla_\theta f(x^{k},\theta^{k+1})=0$\\
 \eIf{$k \enskip \text{mod} \enskip M = 0$}
  {
  Let $(i(k), j(k))$ be the MVP\\ 
  }
  {
  Let $(i(k), j(k))$ be a violating pair\\ 
  }
  Compute $\displaystyle x_{i(k)},x_{j(k)}\in \arg \min_{\xi,\zeta} f(\xi,\zeta)+\frac{1}{2}\tau ||(\xi,\zeta)-(x_{i(k)}^k,x_{j(k)}^k)||^2$\\
  Set $k = k + 1$
 }
 \caption{Decomposition Algorithm with proximal point}
 \label{alg:proximal}
\end{algorithm}
We can prove the following global convergence results.
\begin{proposition}
Suppose that the level set $\mathcal{L}_0$ is a compact set. Let $\{(x^k, \theta^k)\}$ be the sequence of points generated by the decomposition algorithm with proximal point modification. Then
\begin{itemize}
\item $(x^k, \theta^k) \in \mathcal{L}_0, \enskip \forall k$ 
\item  $\{(x^k, \theta^k)\}$ admits limit points and each limit point is critical for Problem (\ref{eq:rpfordec})
\end{itemize}
\end{proposition}
\begin{proof}
The firts assertion is given by algorithm statement which say:
\begin{equation}
f(x^{k+1},\theta^{k+1})\le f(x^{k+1},y^k)\le f(x^k,\theta^k)- \underbrace{\frac{1}{2}\tau ||x^{k+1}-x^{k}||^2}_{>0}
\end{equation}
then the sequence $\{(x^k,\theta^k)\} \in \mathcal{L}_0$.

To prove the second assertion it is necessary to show that
\begin{equation}
\lim_{k \rightarrow \infty} ||(x^{k+1},\theta^{k+1})-(x^{k},y^{k})||=0
\end{equation}
then the convergence can be completed as in previous proposition.

Let us start to prove that $||x^{k+1}-x^{k}||\to0$.

From the $\mathcal{L}_0$ compactness we can say that the sequence $\{(x^k,\theta^k)\}$ has a subsequence $K \subset \{0,1,\ldots\}$ convergent to $(\overline{x},\overline{y}) \in \mathcal{L}_0$. Then for the continuity of $f$ we have:
\begin{equation}
\lim_{k \in K, k \rightarrow \infty}f(x^k,\theta^k)=f(\overline{x},\overline{\theta})= \overline{f}
\end{equation}

The instruction of algorithm imply that:
\begin{equation}
f(x^{k+1},\theta^{k+1})\le f(x^{k+1},\theta^{k})\le f(x^k,\theta^k)
\end{equation}
then the sequence $f(x^k,\theta^k)$ is decreasing and lower-bounded by $\overline{f}$ then we have:
\begin{equation}
\lim_{k \in K, k \rightarrow \infty} f(x^{k+1},\theta^{k})-f(x^{k},\theta^{k})=0
\end{equation}

Proximal point step imply that:
\begin{equation}
f(x^{k+1},y^{k})+\frac{1}{2}\tau||x^{k+1}-x^{k}||^2 \le f(x^{k},y^{k})
\end{equation}

Taking the limit for $k \in K, k \to \infty$ we obtain:
\begin{equation}
\lim_{k \in K,k \to \infty} ||x^{k+1}-x^{k}||=0
\end{equation}

Now we have to prove that $|\theta^{k+1}-\theta^{k}|\to 0$, using the property that $f$ is quadratic and strictly convex with respect to $\theta$. 
We have to prove the following lemma:
\begin{lemma}
Let $f(x,\theta)$ defined in (\ref{eq:rpfordec}), then\\ if $\lim_{k \to \infty} f(x^{k},\theta^k)-f(x^{k},\theta^{k+1})=0$ then:
\begin{equation}
 \lim_{k\to \infty} |\theta^{k+1} -\theta^k| = 0 
\end{equation}
\end{lemma}

\begin{proof}
The instructions of the algorithm imply that at every iteration $k$, we have to select the optimal step $\alpha_k^*$ along $-\nabla_{\theta}f(x^{k},\theta^{k})$.

Because of $f$ is strictly convex quadratic function with respect to $\theta$ we have:
\begin{equation}\label{eq:quadratic}
f(x^{k},\theta^k-\alpha^*_k\nabla_{\theta}f(x^{k},\theta^{k})) = f(x^{k},\theta^{k})-\alpha^*_k |\nabla_{\theta}f(x^{k},\theta^{k})|^2
\end{equation}
with:
\begin{equation}\label{eq:upTheta}
 \theta^{k+1}= \theta^{k}-\alpha^*_k\nabla_{\theta}f(x^{k},\theta^{k})
\end{equation}

From hypothesis we have that:
\begin{equation}
 \lim_{k \to \infty} f(x^{k+1},\theta^k)-f(x^{k+1},\theta^{k+1})=0
 \end{equation}

and from (\ref{eq:upTheta}) and (\ref{eq:quadratic}):
\begin{equation}
  \lim_{k\to \infty} |\theta^{k+1} -\theta^k| = 0 
\end{equation}
\end{proof}
 
Next convergence steps are equal to previous proposition.
\end{proof}

\begin{oss}
In computational experiments we use $\tau = 0$ and compute an exact line search along direction $d^{i(k),j(k)}$. In next section we will show that in practical cases exact line search performs very well especially increasing number of variables.
\end{oss}

\subsection{Implementation guidelines}
First of all, we can write the derivative of the objective function of Problem (\ref{eq:rpfordec}) with respect to $x$ and $\theta$ as:
\begin{equation}\label{eq:gradxdec}
\frac{\partial f(x,\theta)}{\partial x_j} = 2 \sum_{i=1}^n \left(x_i (\Sigma x)_i - \theta\right)\Sigma_{ij}x_i + 2(x_j (\Sigma x)_j - \theta)(\Sigma x)_j
\end{equation}
and
\begin{equation}\label{eq:gradthetadec}
\nabla_{\theta} f(x,\theta)= -2\sum_{i=1}^n \left(x_i (\Sigma x)_i - \theta\right)
\end{equation}
\subsubsection{Matrix product}
As we can see from (\ref{eq:gradxdec}) and (\ref{eq:gradthetadec}), at every iteration $k$ we must compute:
\begin{equation}
\Sigma  x^{k}
\end{equation}
This scalar product is highly time consuming and we therefore use another approach to compute it. At every iteration, only 2 variables changes value, more precisely:
\begin{equation}
x_{i(k)}^{k+1} = x_{i(k)}^{k} + \alpha^{k}
\end{equation}
\begin{equation}
x_{j(k)}^{k+1} = x_{j(k)}^{k} - \alpha^{k}
\end{equation}
Thanks to this updating scheme, we can write:
\begin{equation}
\Sigma x^{k+1} = \Sigma  x^{k} + \left[\Sigma_{i(k)} - \Sigma_{j(k)}\right]\alpha^{k} 
\end{equation}
where $\Sigma_{i}$ is the $i$-th column of $\Sigma$. This shrewdness dramatically reduces the computation time of the algorithm, especially for large values of $n$.

\subsubsection{Gradient computation}
We can split the gradient (\ref{eq:gradxdec}) computation in two parts. The first is:
\begin{equation}\label{eq:first1}
2 \sum_{i=1}^n \left(x_i (\Sigma x)_i - \theta\right)\Sigma_{ij}x_i
\end{equation}
while the second is:
\begin{equation}\label{eq:second}
2(x_j (\Sigma x)_j - \theta)(\Sigma x)_j
\end{equation}
Let us introduce the matrix $B \in \mathbb{R}^{n \times n}$ with elements:
\begin{equation}
B_{i,j} = \Sigma_{i,j}  x_i 
\end{equation}
and the vector $d \in \mathbb{R}^n$ with elements:
\begin{equation}
d_j = 2 (x_j (\Sigma x)_j - \theta) 
\end{equation}
We can now write the two terms of the gradient using $B$ and $d$; the first term (\ref{eq:first1}) becomes:
\begin{equation}
B d
\end{equation}
The second term (\ref{eq:second}) is simply:
\begin{equation}
d_j (\Sigma x)_j
\end{equation}
Finally, we have:
\begin{equation}\label{eq:newgradx}
\frac{\partial F(x,\theta)}{\partial x_j} =\left(B d\right)_j + d_j (\Sigma x)_j
\end{equation}
At every iteration $k$, we have:
\begin{equation}
B^{k}_{h} = \begin{cases}
		  		\Sigma_{h}x_h^{k} \quad \text{if } h = i(k) \vee h = j(k)\\\\
		  	    B^{k-1}_{h} \quad \text{     otherwise}
		  		\end{cases}
\end{equation}
where $B_{h}$ is the $h$-th column of $B$.

\subsubsection{Violating Pair selection}
In Algorithm \ref{alg:proximal} we choose a generic violating pair, without no assumptions on the \textquotedblleft quality " of this pair. In practice, we select a \textquotedblleft sufficient" violating pair instead, selecting the MVP among a subset of asset's indexes $\left\{1,.., n\right\}$. For each iteration $k$, let
\begin{equation}\label{eq:wk}
W^k \subset \left\{1,.., n\right\}
\end{equation}
a random subset of the asset's indexes such that
\begin{equation}\label{eq:wk2}
\frac{|W^k|}{n} \times 100 = \lambda
\end{equation}
Algorithm \ref{alg:modified} summarizes what discussed above.

\begin{algorithm}[ht]
 \KwData{Given the initial feasible point $(x^{0}, \theta^{0})$ and $\lambda$}
 Set $k = 0$\\
 \While{(not convergence)}{
   Compute $\theta^{k+1}$ such that $f(x^{k},\theta^{k+1})\le f(x^{k},\theta^k)$ and $\nabla_\theta f(x^{k},\theta^{k+1})=0$\\
 \eIf{$k \enskip \text{mod} \enskip M = 0$}
  {
  Let $(i(k), j(k))$ be the MVP\\ 
  }
  {
  Choose a random subset $W^k$ such that (\ref{eq:wk2}) holds\\
  Let $(i(k), j(k))$ be the MVP among indexes in $W^k$\\ 
  }
  Compute $\displaystyle x_{i(k)},x_{j(k)}\in \arg \min_{\xi,\zeta} f(\xi,\zeta)+\frac{1}{2}\tau ||(\xi,\zeta)-(x_{i(k)}^k,x_{j(k)}^k)||^2$\\
  Set $k = k + 1$
 }
 \caption{Decomposition Algorithm with $\lambda$}
 \label{alg:modified}
\end{algorithm}
